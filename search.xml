<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>使用自己的语料训练word2vec模型</title>
    <url>/2019/12/12/1/</url>
    <content><![CDATA[<h4 id="使用自己的语料训练word2vec模型"><a href="#使用自己的语料训练word2vec模型" class="headerlink" title="使用自己的语料训练word2vec模型"></a><strong>使用自己的语料训练word2vec模型</strong></h4><h5 id="一、准备环境和语料："><a href="#一、准备环境和语料：" class="headerlink" title="一、准备环境和语料："></a>一、准备环境和语料：</h5><p>新闻20w+篇（格式：<code>标题</code>。<code>正文</code>） </p>
<p>【新闻可以自己从各大新闻网站爬取，也可以下载开源的新闻数据集，如 </p>
<ul>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5zb2dvdS5jb20vbGFicy9yZXNvdXJjZS90LnBocA==" title="http://www.sogou.com/labs/resource/t.php">互联网语料库(SogouT)<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3RodWN0Yy50aHVubHAub3JnLw==" title="http://thuctc.thunlp.org/">中文文本分类数据集THUCNews<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk2OA==" title="http://www.datatang.com/data/11968">李荣陆英文文本分类语料<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk3MA==" title="http://www.datatang.com/data/11970">谭松波中文文本分类语料<i class="fa fa-external-link"></i></span></p>
</li>
<li><p>等</p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z4c2p5L2ppZWJh" title="https://github.com/fxsjy/jieba">结巴分词<i class="fa fa-external-link"></i></span> </p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9yYWRpbXJlaHVyZWsuY29tL2dlbnNpbS9tb2RlbHMvd29yZDJ2ZWMuaHRtbA==" title="https://radimrehurek.com/gensim/models/word2vec.html">word2vec<i class="fa fa-external-link"></i></span></p>
</li>
</ul>
<h5 id="二、分词"><a href="#二、分词" class="headerlink" title="二、分词"></a>二、分词</h5><p>先对新闻文本进行分词，使用的是结巴分词工具，将分词后的文本保存在<code>seg201708.txt</code>，以备后期使用。</p>
<blockquote>
<p>   安装jieba工具包：pip install jieba </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># 加载自己的自己的金融词库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">"financialWords.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">with</span> io.open(<span class="string">'news201708.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            seg_list = jieba.cut(line)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#           print '/'.join(seg_list)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">with</span> io.open(<span class="string">'seg201708.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                output.write(<span class="string">' '</span>.join(seg_list))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>

<a id="more"></a>

<h5 id="三、训练word2vec模型"><a href="#三、训练word2vec模型" class="headerlink" title="三、训练word2vec模型"></a>三、训练word2vec模型</h5><p>使用python的gensim包进行训练。</p>
<blockquote>
<p>   安装gemsim包：pip install gemsim </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_features = <span class="number">300</span>    <span class="comment"># Word vector dimensionality</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    min_word_count = <span class="number">10</span>   <span class="comment"># Minimum word count</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_workers = <span class="number">16</span>       <span class="comment"># Number of threads to run in parallel</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    context = <span class="number">10</span>          <span class="comment"># Context window size</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    downsampling = <span class="number">1e-3</span>   <span class="comment"># Downsample setting for frequent words</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    sentences = word2vec.Text8Corpus(<span class="string">"seg201708.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="code"><pre><span class="line">    model = word2vec.Word2Vec(sentences, workers=num_workers, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            size=num_features, min_count = min_word_count, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            window = context, sg = <span class="number">1</span>, sample = downsampling)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.init_sims(replace=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 保存模型，供日後使用</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.save(<span class="string">"model201708"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 可以在加载模型之后使用另外的句子来进一步训练模型</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model = gensim.models.Word2Vec.load('/tmp/mymodel')</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model.train(more_sentences)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>

<ul>
<li>参数说明</li>
</ul>
<blockquote>
</blockquote>
<ul>
<li>sentences：可以是一个·ist，对于大语料集，建议使用BrownCorpus,Text8Corpus或ineSentence构建。</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。</li>
<li>size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。</li>
<li>window：表示当前词与预测词在一个句子中的最大距离是多少</li>
<li>alpha: 是学习速率</li>
<li>seed：用于随机数发生器。与初始化词向量有关。</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)</li>
<li>workers参数控制训练的并行数。</li>
<li>hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。</li>
<li>negative: 如果&gt;0,则会采用negativesamp·ing，用于设置多少个noise words</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数</li>
<li>iter： 迭代次数，默认为5</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的</li>
<li>sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。</li>
<li>batch_words：每一批的传递给线程的单词的数量，默认为10000</li>
</ul>
<h5 id="四、word2vec应用"><a href="#四、word2vec应用" class="headerlink" title="四、word2vec应用"></a>四、word2vec应用</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Word2Vec.load(<span class="string">'model201708'</span>)      <span class="comment">#模型讀取方式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.most_similar(positive=[<span class="string">'woman'</span>, <span class="string">'king'</span>], negative=[<span class="string">'man'</span>]) <span class="comment">#根据给定的条件推断相似词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.doesnt_match(<span class="string">"breakfast cereal dinner lunch"</span>.split()) <span class="comment">#寻找离群词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.similarity(<span class="string">'woman'</span>, <span class="string">'man'</span>) <span class="comment">#计算两个单词的相似度</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model[<span class="string">'computer'</span>] <span class="comment">#获取单词的词向量</span></span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Word</title>
    <url>/2019/12/11/1/</url>
    <content><![CDATA[<p> <img src="https://ftp.bmp.ovh/imgs/2019/12/46248c3148c47ae2.png" alt=""> </p>
]]></content>
  </entry>
</search>
