<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HMM 模型代码实现</title>
    <url>/2019/12/17/1/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainHmm</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,TrainingFile,TestFile)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.train_set = TrainingFile</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.test_set = TestFile</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.training()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.testing()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findIndex</span><span class="params">(self,i,lens)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">if</span> lens == <span class="number">1</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="string">'S'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> :</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="string">'B'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">if</span> i == lens<span class="number">-1</span> :</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="string">'E'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">return</span> <span class="string">'M'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training</span><span class="params">(self)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> <span class="string">'training ...'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        a = <span class="string">'  '</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpo = open(self.train_set,<span class="string">'r'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.count = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.cnt = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.words = list()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        strs = <span class="string">""</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fpo:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            line = line.decode(<span class="string">'utf8'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            line = line.replace(<span class="string">' \n'</span>,<span class="string">''</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            line = line.replace(<span class="string">'\n'</span>,<span class="string">''</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            grap = line.split(a)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> scen <span class="keyword">in</span> grap:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                scen = scen.strip()</span></pre></td></tr><tr><td class="code"><pre><span class="line">                lens = len(scen)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>,lens):</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    wd = scen[i]</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    <span class="keyword">if</span> wd <span class="keyword">not</span> <span class="keyword">in</span> self.words:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        self.words.append(wd)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    st = self.findIndex(i,lens)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.cnt.setdefault(st,<span class="number">0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.cnt[st] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                    strs += st</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.count.setdefault(st,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.count[st].setdefault(wd,<span class="number">0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.count[st][wd] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            strs += <span class="string">','</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpo.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        ast = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(strs)<span class="number">-2</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">            st1 = strs[i]</span></pre></td></tr><tr><td class="code"><pre><span class="line">            st2 = strs[i+<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">if</span> st1 == <span class="string">','</span> <span class="keyword">or</span> st2 == <span class="string">','</span> :</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">continue</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            ast.setdefault(st1,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            ast[st1].setdefault(st2,<span class="number">0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            ast[st1][st2] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.pi = &#123;<span class="string">'B'</span>:<span class="number">0.5</span>,<span class="string">'M'</span>:<span class="number">0</span>,<span class="string">'E'</span>:<span class="number">0</span>,<span class="string">'S'</span>:<span class="number">0.5</span>&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.matrixA = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        self.matrixB = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        state = [<span class="string">'B'</span>,<span class="string">'M'</span>,<span class="string">'E'</span>,<span class="string">'S'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> st1 <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            self.matrixA.setdefault(st1,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> st2 <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                self.matrixA[st1].setdefault(st2,<span class="number">0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> st1,item <span class="keyword">in</span> ast.items():</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> st2 <span class="keyword">in</span> item.keys():</span></pre></td></tr><tr><td class="code"><pre><span class="line">                self.matrixA[st1][st2] = float(item[st2])/float(self.cnt[st1])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> st <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            self.matrixB.setdefault(st,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> wd <span class="keyword">in</span> self.words:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                self.matrixB[st].setdefault(wd,<span class="number">1.0</span>/float(self.cnt[st]))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> st,item <span class="keyword">in</span> self.count.items():</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> wd <span class="keyword">in</span> item.keys():</span></pre></td></tr><tr><td class="code"><pre><span class="line">                self.matrixB[st][wd] = float(item[wd])/float(self.cnt[st])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> <span class="string">'training completed'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">testing</span><span class="params">(self)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> <span class="string">'testing ...'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        filename,_ = self.test_set.split(<span class="string">'.'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        filename += <span class="string">'_result.utf8'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpo = open(self.test_set,<span class="string">'r'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpw = open(filename,<span class="string">'w'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        fi = dict()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        state = [<span class="string">'B'</span>,<span class="string">'E'</span>,<span class="string">'M'</span>,<span class="string">'S'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        num = <span class="number">0</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> eachline <span class="keyword">in</span> fpo:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            num += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            line = eachline.decode(<span class="string">'utf8'</span>).strip()</span></pre></td></tr><tr><td class="code"><pre><span class="line">            lens = len(line)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">if</span> lens &lt; <span class="number">1</span> :</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">continue</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            wd = line[<span class="number">0</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> st <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                fi.setdefault(<span class="number">1</span>,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">if</span> wd <span class="keyword">not</span> <span class="keyword">in</span> self.matrixB[st].keys():</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    self.matrixB[st].setdefault(wd,<span class="number">1.0</span>/float(self.cnt[st]))</span></pre></td></tr><tr><td class="code"><pre><span class="line">                fi[<span class="number">1</span>].setdefault(st,self.pi[st]*self.matrixB[st][wd])</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>,lens):</span></pre></td></tr><tr><td class="code"><pre><span class="line">                wd = line[i]</span></pre></td></tr><tr><td class="code"><pre><span class="line">                fi.setdefault(i+<span class="number">1</span>,&#123;&#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">for</span> st1 <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    fi[i+<span class="number">1</span>].setdefault(st1,<span class="number">0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    max_num = <span class="number">0</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                    <span class="keyword">for</span> st2 <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        max_num = max(max_num,fi[i][st2]*self.matrixA[st2][st1])</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    <span class="keyword">if</span> wd <span class="keyword">not</span> <span class="keyword">in</span> self.matrixB[st1].keys():</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        self.matrixB[st1][wd] = <span class="number">1.0</span>/float(self.cnt[st1])</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    fi[i+<span class="number">1</span>][st1] = max_num*self.matrixB[st1][wd]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">            links = list()</span></pre></td></tr><tr><td class="code"><pre><span class="line">            tmp = list()</span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> st <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                tmp.append([st,fi[lens][st]])</span></pre></td></tr><tr><td class="code"><pre><span class="line">            st1,_ = max(tmp,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">            links.append(st1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(lens,<span class="number">1</span>,<span class="number">-1</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">                tmp = list()</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">for</span> st <span class="keyword">in</span> state:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    tmp.append([st,fi[i<span class="number">-1</span>][st]*self.matrixA[st][st1]])</span></pre></td></tr><tr><td class="code"><pre><span class="line">                st1,sc = max(tmp,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">                links.append(st1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">            links.reverse()</span></pre></td></tr><tr><td class="code"><pre><span class="line">            strs = <span class="string">""</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(links)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">                st = links[i]</span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">if</span> st == <span class="string">'S'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    strs += (line[i]+<span class="string">'  '</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    <span class="keyword">continue</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">if</span> st == <span class="string">'B'</span> <span class="keyword">or</span> st == <span class="string">'M'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    strs += line[i]</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    <span class="keyword">continue</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                <span class="keyword">if</span> st == <span class="string">'E'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    strs += (line[i]+<span class="string">'  '</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">            strs += <span class="string">'\n'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            fpw.writelines(strs.encode(<span class="string">'utf8'</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpo.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        fpw.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> <span class="string">'test completed'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    args = len(sys.argv)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span>(args &lt; <span class="number">3</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> <span class="string">"Usage [trainingSet] [testSet] for utf-8"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        sys.exit(<span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    hmm_train = TrainHmm(sys.argv[<span class="number">1</span>],sys.argv[<span class="number">2</span>])</span></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>jieba分词笔记</title>
    <url>/2019/12/17/1/</url>
    <content><![CDATA[<h2 id="0-引言"><a href="#0-引言" class="headerlink" title="0 引言"></a>0 引言</h2><p>  <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z4c2p5L2ppZWJh" title="https://github.com/fxsjy/jieba">jieba<i class="fa fa-external-link"></i></span> 是目前最好的 Python 中文分词组件，它主要有以下 3 种特性：</p>
<ul>
<li>支持 3 种分词模式：精确模式、全模式、搜索引擎模式</li>
<li>支持繁体分词</li>
<li>支持自定义词典</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入 jieba</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg <span class="comment">#词性标注</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> anls <span class="comment">#关键词提取</span></span></pre></td></tr></table></figure>

<h2 id="1-分词"><a href="#1-分词" class="headerlink" title="1 分词"></a>1 分词</h2><p>  可使用 <code>jieba.cut</code> 和 <code>jieba.cut_for_search</code> 方法进行分词，两者所返回的结构都是一个<strong>可迭代</strong>的 generator，可使用 for 循环来获得分词后得到的每一个词语（unicode），或者直接使用 <code>jieba.lcut</code> 以及 <code>jieba.lcut_for_search</code> 直接返回 list。其中：</p>
<ul>
<li><pre><code>jieba.cut
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">和</span></pre></td></tr></table></figure>
jieba.lcut
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">   接受 3 个参数： </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">  - 需要分词的字符串（unicode 或 UTF-8 字符串、GBK 字符串）</span></pre></td></tr><tr><td class="code"><pre><span class="line">  - cut_all 参数：是否使用全模式，默认值为 &#96;False&#96; </span></pre></td></tr><tr><td class="code"><pre><span class="line">  - HMM 参数：用来控制是否使用 HMM 模型，默认值为 &#96;True&#96; </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">-</span></pre></td></tr></table></figure>
jieba.cut_for_search
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">和</span></pre></td></tr></table></figure>
jieba.lcut_for_search
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">   接受 2 个参数： </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">  - 需要分词的字符串（unicode 或 UTF-8 字符串、GBK 字符串）</span></pre></td></tr><tr><td class="code"><pre><span class="line">  - HMM 参数：用来控制是否使用 HMM 模型，默认值为 &#96;True&#96; </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">*# 尽量不要使用 GBK 字符串，可能无法预料地错误解码成 UTF-8*</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">### 1.1 全模式和精确模式</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">&#96;&#96;&#96;python</span></pre></td></tr><tr><td class="code"><pre><span class="line"># 全模式</span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list &#x3D; jieba.cut(&quot;他来到上海交通大学&quot;, cut_all&#x3D;True)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(&quot;【全模式】：&quot; + &quot;&#x2F; &quot;.join(seg_list))</span></pre></td></tr></table></figure>
</code></pre></li>
</ul>
<blockquote>
<p>【全模式】：他/ 来到/ 上海/ 上海交通大学/ 交通/ 大学</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 精确模式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.cut(<span class="string">"他来到上海交通大学"</span>, cut_all=<span class="literal">False</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【精确模式】："</span> + <span class="string">"/ "</span>.join(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【精确模式】：他/ 来到/ 上海交通大学</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(seg_list)</span></pre></td></tr></table></figure>

<blockquote>
<p>generator</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回列表</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.lcut(<span class="string">"他来到上海交通大学"</span>, cut_all=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【返回列表】：&#123;0&#125;"</span>.format(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【返回列表】：[‘他’, ‘来到’, ‘上海’, ‘上海交通大学’, ‘交通’, ‘大学’]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(seg_list)</span></pre></td></tr></table></figure>

<blockquote>
<p>list</p>
</blockquote>
<h3 id="1-2-搜索引擎模式"><a href="#1-2-搜索引擎模式" class="headerlink" title="1.2 搜索引擎模式"></a>1.2 搜索引擎模式</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索引擎模式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.cut_for_search(<span class="string">"他毕业于上海交通大学机电系，后来在一机部上海电器科学研究所工作"</span>)  </span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【搜索引擎模式】："</span> + <span class="string">"/ "</span>.join(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【搜索引擎模式】：他/ 毕业/ 于/ 上海/ 交通/ 大学/ 上海交通大学/ 机电/ 系/ ，/ 后来/ 在/ 一机部/ 上海/ 电器/ 科学/ 研究/ 研究所/ 工作</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回列表</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.lcut_for_search(<span class="string">"他毕业于上海交通大学机电系，后来在一机部上海电器科学研究所工作"</span>)  </span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【返回列表】：&#123;0&#125;"</span>.format(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【返回列表】：[‘他’, ‘毕业’, ‘于’, ‘上海’, ‘交通’, ‘大学’, ‘上海交通大学’, ‘机电’, ‘系’, ‘，’, ‘后来’, ‘在’, ‘一机部’, ‘上海’, ‘电器’, ‘科学’, ‘研究’, ‘研究所’, ‘工作’]</p>
</blockquote>
<h3 id="1-3-HMM-模型"><a href="#1-3-HMM-模型" class="headerlink" title="1.3 HMM 模型"></a>1.3 HMM 模型</h3><p>  HMM 模型，即<strong>隐马尔可夫模型（Hidden Markov Model, HMM）</strong>，是一种基于概率的统计分析模型，用来描述一个系统隐性状态的转移和隐性状态的表现概率。在 jieba 中，对于未登录到词库的词，使用了基于汉字成词能力的 HMM 模型和 Viterbi 算法，其大致原理是：</p>
<blockquote>
<p>采用四个隐含状态，分别表示为单字成词，词组的开头，词组的中间，词组的结尾。通过标注好的分词训练集，可以得到 HMM 的各个参数，然后使用 Viterbi 算法来解释测试集，得到分词结果。</p>
</blockquote>
<p><em># 代码实现可参考 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xlZXNoaW5lL1dvcmRTZWcvYmxvYi9tYXN0ZXIvc3JjL0htbS9IbW1TZWcucHk=" title="https://github.com/Leeshine/WordSeg/blob/master/src/Hmm/HmmSeg.py">HmmSeg.py<i class="fa fa-external-link"></i></span></em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 未启用 HMM</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>, HMM=<span class="literal">False</span>) <span class="comment">#默认精确模式和启用 HMM</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【未启用 HMM】："</span> + <span class="string">"/ "</span>.join(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【未启用 HMM】：他/ 来到/ 了/ 网易/ 杭/ 研/ 大厦</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 识别新词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>) <span class="comment">#默认精确模式和启用 HMM</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【识别新词】："</span> + <span class="string">"/ "</span>.join(seg_list))</span></pre></td></tr></table></figure>

<blockquote>
<p>【识别新词】：他/ 来到/ 了/ 网易/ 杭研/ 大厦</p>
</blockquote>
<h2 id="2-繁体字分词"><a href="#2-繁体字分词" class="headerlink" title="2 繁体字分词"></a>2 繁体字分词</h2><p>  jieba 还支持对繁体字进行分词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 繁体字文本</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">ft_text = <span class="string">"""人生易老天難老 歲歲重陽 今又重陽 戰地黃花分外香 壹年壹度秋風勁 不似春光 勝似春光 寥廓江天萬裏霜 """</span></span></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 全模式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【全模式】："</span> + <span class="string">"/ "</span>.join(jieba.cut(ft_text, cut_all=<span class="literal">True</span>)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【全模式】：人生/ 易/ 老天/ 難/ 老/ / / 歲/ 歲/ 重/ 陽/ / / 今/ 又/ 重/ 陽/ / / 戰/ 地/ 黃/ 花/ 分外/ 香/ / / 壹年/ 壹/ 度/ 秋/ 風/ 勁/ / / 不似/ 春光/ / / 勝/ 似/ 春光/ / / 寥廓/ 江天/ 萬/ 裏/ 霜/ /</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 精确模式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【精确模式】："</span> + <span class="string">"/ "</span>.join(jieba.cut(ft_text, cut_all=<span class="literal">False</span>)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【精确模式】：人生/ 易/ 老天/ 難老/  / 歲/ 歲/ 重陽/  / 今/ 又/ 重陽/  / 戰地/ 黃/ 花/ 分外/ 香/  / 壹年/ 壹度/ 秋風勁/  / 不/ 似/ 春光/  / 勝似/ 春光/  / 寥廓/ 江天/ 萬/ 裏/ 霜/</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索引擎模式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【搜索引擎模式】："</span> + <span class="string">"/ "</span>.join(jieba.cut_for_search(ft_text)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【搜索引擎模式】：人生/ 易/ 老天/ 難老/  / 歲/ 歲/ 重陽/  / 今/ 又/ 重陽/  / 戰地/ 黃/ 花/ 分外/ 香/  / 壹年/ 壹度/ 秋風勁/  / 不/ 似/ 春光/  / 勝似/ 春光/  / 寥廓/ 江天/ 萬/ 裏/ 霜/</p>
</blockquote>
<h2 id="3-添加自定义词典"><a href="#3-添加自定义词典" class="headerlink" title="3 添加自定义词典"></a>3 添加自定义词典</h2><p>  开发者可以指定自定义词典，以便包含 jieba 词库里没有的词，词典格式如下：</p>
<blockquote>
<p>词语 词频（可省略） 词性（可省略）</p>
</blockquote>
<p>  例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">创新办 3 i</span></pre></td></tr><tr><td class="code"><pre><span class="line">云计算 5</span></pre></td></tr><tr><td class="code"><pre><span class="line">凱特琳 nz</span></pre></td></tr></table></figure>

<p><em># 虽然  jieba 有新词识别能力，但自行添加新词可以保证更高的正确率。</em></p>
<h3 id="3-1-载入词典"><a href="#3-1-载入词典" class="headerlink" title="3.1 载入词典"></a>3.1 载入词典</h3><p>  使用 <code>jieba.load_userdict(file_name)</code> 即可载入词典。</p>
<p><em># <code>file_name</code> 为文件类对象或自定义词典的路径</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 示例文本</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">sample_text = <span class="string">"周大福是创新办主任也是云计算方面的专家"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># 未加载词典</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【未加载词典】："</span> + <span class="string">'/ '</span>.join(jieba.cut(sample_text)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【未加载词典】：周大福/ 是/ 创新/ 办/ 主任/ 也/ 是/ 云/ 计算/ 方面/ 的/ 专家</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 载入词典</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">"userdict.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># 加载词典后</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【加载词典后】："</span> + <span class="string">'/ '</span>.join(jieba.cut(sample_text)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【加载词典后】：周大福/ 是/ 创新办/ 主任/ 也/ 是/ 云计算/ 方面/ 的/ 专家</p>
</blockquote>
<h3 id="3-2-调整词典"><a href="#3-2-调整词典" class="headerlink" title="3.2 调整词典"></a>3.2 调整词典</h3><p>  使用 <code>add_word(word, freq=None, tag=None)</code> 和 <code>del_word(word)</code> 可在程序中动态修改词典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">jieba.add_word(<span class="string">'石墨烯'</span>) <span class="comment">#增加自定义词语</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.add_word(<span class="string">'凱特琳'</span>, freq=<span class="number">42</span>, tag=<span class="string">'nz'</span>) <span class="comment">#设置词频和词性 </span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.del_word(<span class="string">'自定义词'</span>) <span class="comment">#删除自定义词语</span></span></pre></td></tr></table></figure>

<p>  使用 <code>suggest_freq(segment, tune=True)</code> 可调节单个词语的词频，使其能（或不能）被分出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调节词频前</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【调节词频前】："</span> + <span class="string">'/'</span>.join(jieba.cut(<span class="string">'如果放到post中将出错。'</span>, HMM=<span class="literal">False</span>)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【调节词频前】：如果/放到/post/中将/出错/。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调节词频</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.suggest_freq((<span class="string">'中'</span>, <span class="string">'将'</span>), <span class="literal">True</span>)</span></pre></td></tr></table></figure>

<blockquote>
<p>494</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调节词频后</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【调节词频后】："</span> + <span class="string">'/'</span>.join(jieba.cut(<span class="string">'如果放到post中将出错。'</span>, HMM=<span class="literal">False</span>)))</span></pre></td></tr></table></figure>

<blockquote>
<p>【调节词频后】：如果/放到/post/中/将/出错/。</p>
</blockquote>
<h2 id="4-关键词提取"><a href="#4-关键词提取" class="headerlink" title="4 关键词提取"></a>4 关键词提取</h2><p>  jieba 提供了两种关键词提取方法，分别基于 TF-IDF 算法和 TextRank 算法。</p>
<h3 id="4-1-基于-TF-IDF-算法的关键词提取"><a href="#4-1-基于-TF-IDF-算法的关键词提取" class="headerlink" title="4.1 基于 TF-IDF 算法的关键词提取"></a>4.1 基于 TF-IDF 算法的关键词提取</h3><p>  <strong>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)</strong>是一种统计方法，用以评估一个词语对于一个文件集或一个语料库中的一份文件的重要程度，其原理可概括为：</p>
<blockquote>
<p>一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，越能够代表该文章</p>
</blockquote>
<p>  计算公式：TF-IDF = TF * IDF，其中：</p>
<ul>
<li><p>TF(term frequency, TF)：词频，某一个给定的词语在该文件中出现的次数，计算公式：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6533825-c69c15bcacb4ebbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/250/format/webp" alt="img"></p>
</li>
<li><p>IDF(inverse document frequency, IDF)：逆文件频率，如果包含词条的文件越少，则说明词条具有很好的类别区分能力，计算公式：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6533825-a6a941376dfd68b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/252/format/webp" alt="img"></p>
</li>
</ul>
<p>  通过 <code>jieba.analyse.extract_tags</code> 方法可以基于 TF-IDF 算法进行关键词提取，该方法共有 4 个参数：</p>
<ul>
<li>sentence：为待提取的文本</li>
<li>topK：为返回几个 TF/IDF 权重最大的关键词，默认值为 20</li>
<li>withWeight：是否一并返回关键词权重值，默认值为 False</li>
<li>allowPOS：仅包括指定词性的词，默认值为空</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = <span class="string">"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, w <span class="keyword">in</span> anls.extract_tags(s, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">'%s %s'</span> % (x, w))</span></pre></td></tr></table></figure>

<blockquote>
<p>欧亚 0.7300142700289363<br> 吉林 0.659038184373617<br> 置业 0.4887134522112766<br> 万元 0.3392722481859574<br> 增资 0.33582401985234045<br> 4.3 0.25435675538085106<br> 7000 0.25435675538085106<br> 2013 0.25435675538085106<br> 139.13 0.25435675538085106<br> 实现 0.19900979900382978<br> 综合体 0.19480309624702127<br> 经营范围 0.19389757253595744<br> 亿元 0.1914421623587234<br> 在建 0.17541884768425534<br> 全资 0.17180164988510638<br> 注册资本 0.1712441526<br> 百货 0.16734460041382979<br> 零售 0.1475057117057447<br> 子公司 0.14596045237787234<br> 营业 0.13920178509021275</p>
</blockquote>
<p>  使用 <code>jieba.analyse.TFIDF(idf_path=None)</code> 可以新建 TFIDF 实例，其中 <code>idf_path</code> 为 IDF 频率文件。</p>
<h3 id="4-2-基于-TextRank-算法的关键词提取"><a href="#4-2-基于-TextRank-算法的关键词提取" class="headerlink" title="4.2 基于 TextRank 算法的关键词提取"></a>4.2 基于 TextRank 算法的关键词提取</h3><p>  TextRank 是另一种关键词提取算法，基于大名鼎鼎的 PageRank，其原理可参见论文—— <span class="exturl" data-url="aHR0cDovL3dlYi5lZWNzLnVtaWNoLmVkdS9+bWloYWxjZWEvcGFwZXJzL21paGFsY2VhLmVtbmxwMDQucGRm" title="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">TextRank: Bringing Order into Texts<i class="fa fa-external-link"></i></span> 。</p>
<p>  通过 <code>jieba.analyse.textrank</code> 方法可以使用基于 TextRank 算法的关键词提取，其与 ‘jieba.analyse.extract_tags’ 有一样的参数，但前者默认过滤词性（<code>allowPOS=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;)</code>）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, w <span class="keyword">in</span> anls.textrank(s, withWeight=<span class="literal">True</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">'%s %s'</span> % (x, w))</span></pre></td></tr></table></figure>

<blockquote>
<p>吉林 1.0<br> 欧亚 0.9966893354178172<br> 置业 0.6434360313092776<br> 实现 0.5898606692859626<br> 收入 0.43677859947991454<br> 增资 0.4099900531283276<br> 子公司 0.35678295947672795<br> 城市 0.34971383667403655<br> 商业 0.34817220716026936<br> 业务 0.3092230992619838<br> 在建 0.3077929164033088<br> 营业 0.3035777049319588<br> 全资 0.303540981053475<br> 综合体 0.29580869172394825<br> 注册资本 0.29000519464085045<br> 有限公司 0.2807830798576574<br> 零售 0.27883620861218145<br> 百货 0.2781657628445476<br> 开发 0.2693488779295851<br> 经营范围 0.2642762173558316</p>
</blockquote>
<p>  使用 <code>jieba.analyse.TextRank()</code> 可以新建自定义 TextRank 实例。</p>
<h3 id="4-3-自定义语料库"><a href="#4-3-自定义语料库" class="headerlink" title="4.3 自定义语料库"></a>4.3 自定义语料库</h3><p>  关键词提取所使用逆向文件频率（IDF）文本语料库和停止词（Stop Words）文本语料库可以切换成自定义语料库的路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">jieba.analyse.set_stop_words(<span class="string">"stop_words.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.analyse.set_idf_path(<span class="string">"idf.txt.big"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, w <span class="keyword">in</span> anls.extract_tags(s, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">'%s %s'</span> % (x, w))</span></pre></td></tr></table></figure>

<blockquote>
<p>吉林 1.0174270215234043<br> 欧亚 0.7300142700289363<br> 增资 0.5087135107617021<br> 实现 0.5087135107617021<br> 置业 0.4887134522112766<br> 万元 0.3392722481859574<br> 此外 0.25435675538085106<br> 全资 0.25435675538085106<br> 有限公司 0.25435675538085106<br> 4.3 0.25435675538085106<br> 注册资本 0.25435675538085106<br> 7000 0.25435675538085106<br> 增加 0.25435675538085106<br> 主要 0.25435675538085106<br> 房地产 0.25435675538085106<br> 业务 0.25435675538085106<br> 目前 0.25435675538085106<br> 城市 0.25435675538085106<br> 综合体 0.25435675538085106<br> 2013 0.25435675538085106</p>
</blockquote>
<h2 id="5-词性标注"><a href="#5-词性标注" class="headerlink" title="5 词性标注"></a>5 词性标注</h2><p>  <code>jieba.posseg.POSTokenizer(tokenizer=None)</code> 新建自定义分词器，<code>tokenizer</code> 参数可指定内部使用的 <code>jieba.Tokenizer</code> 分词器。<code>jieba.posseg.dt</code> 为默认词性标注分词器。</p>
<p><em># 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words = pseg.cut(<span class="string">"他改变了中国"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word, flag <span class="keyword">in</span> words:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"&#123;0&#125; &#123;1&#125;"</span>.format(word, flag))</span></pre></td></tr></table></figure>

<blockquote>
<p>他 r<br>改变 v<br>了 ul<br>中国 ns</p>
</blockquote>
<h2 id="6-并行分词"><a href="#6-并行分词" class="headerlink" title="6 并行分词"></a>6 并行分词</h2><p>  将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升。用法：</p>
<ul>
<li>jieba.enable_parallel(4)：开启并行分词模式，参数为并行进程数</li>
<li>jieba.disable_parallel() ：关闭并行分词模式</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span></pre></td></tr><tr><td class="code"><pre><span class="line">sys.path.append(<span class="string">"../../"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.enable_parallel()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">url = sys.argv[<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">content = open(url,<span class="string">"rb"</span>).read()</span></pre></td></tr><tr><td class="code"><pre><span class="line">t1 = time.time()</span></pre></td></tr><tr><td class="code"><pre><span class="line">words = <span class="string">"/ "</span>.join(jieba.cut(content))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">t2 = time.time()</span></pre></td></tr><tr><td class="code"><pre><span class="line">tm_cost = t2-t1</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">log_f = open(<span class="string">"1.log"</span>,<span class="string">"wb"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">log_f.write(words.encode(<span class="string">'utf-8'</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'speed %s bytes/second'</span> % (len(content)/tm_cost))</span></pre></td></tr></table></figure>



<h2 id="7-返回词语在原文的起止位置"><a href="#7-返回词语在原文的起止位置" class="headerlink" title="7 返回词语在原文的起止位置"></a>7 返回词语在原文的起止位置</h2><p>  使用 <code>jieba.tokenize</code> 方法可以返回词语在原文的起止位置。</p>
<blockquote>
<p>注意：输入参数只接受 unicode</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = jieba.tokenize(<span class="string">u'上海益民食品一厂有限公司'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【普通模式】"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"word: &#123;0&#125; \t\t start: &#123;1&#125; \t\t end: &#123;2&#125;"</span>.format(tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span></pre></td></tr></table></figure>

<blockquote>
<p>【普通模式】<br> word: 上海         start: 0        end: 2<br> word: 益民         start: 2        end: 4<br> word: 食品         start: 4        end: 6<br> word: 一厂         start: 6        end: 8<br> word: 有限公司       start: 8        end: 12</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = jieba.tokenize(<span class="string">u'上海益民食品一厂有限公司'</span>, mode=<span class="string">'search'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">"【搜索模式】"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> tk <span class="keyword">in</span> result:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"word: &#123;0&#125; \t\t start: &#123;1&#125; \t\t end: &#123;2&#125;"</span>.format(tk[<span class="number">0</span>],tk[<span class="number">1</span>],tk[<span class="number">2</span>]))</span></pre></td></tr></table></figure>

<blockquote>
<p>【搜索模式】<br> word: 上海         start: 0        end: 2<br> word: 益民         start: 2        end: 4<br> word: 食品         start: 4        end: 6<br> word: 一厂         start: 6        end: 8<br> word: 有限         start: 8        end: 10<br> word: 公司         start: 10       end: 12<br> word: 有限公司       start: 8        end: 12</p>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Gensim模块训练词向量(维基百科预料)</title>
    <url>/2019/12/13/1/</url>
    <content><![CDATA[<p>如果在以词为基本单元输入的自然语言处理任务中，都避免不了使用词的表示，词的表示有很多种，这里使用的是词向量，word2vec是目前比较通用的训练词向量的工具，使用Gensim模块，可以使词向量的训练变得简单，word2vec常见的两种模型 (CBOW和Skip-Gram) 他们的输入以及输出都是以单词为基本单位的，只是他们对应的输入以及输出不一样： </p>
<ol>
<li><p>Skip-Gram models：输入为单个词，输出目标为多个上下文单词；</p>
</li>
<li><p>CBOW models：输入为多个上下文单词，输出目标为一个单词；</p>
<p>从上面可以看出，无论是Skip-Gram models还是CBOW models基本的单元都是词，那么我们获取到的语料，必须要经过分词处理以后才能用于词向量的训练语料。 </p>
</li>
</ol>
<a id="more"></a>

<h5 id="1-数据的处理"><a href="#1-数据的处理" class="headerlink" title="1.数据的处理"></a>1.数据的处理</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#以写的方式打开原始的简体中文语料库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">f=codecs.open(<span class="string">'zhwiki_jian_zh.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">"utf8"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#将分完词的语料写入到wiki_jian_zh_seg-189.5.txt文件中</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">target = codecs.open(<span class="string">"wiki_jian_zh_seg-189.5.txt"</span>, <span class="string">'w'</span>,encoding=<span class="string">"utf8"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'open files'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">line_num=<span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">line = f.readline()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#循环遍历每一行，并对这一行进行分词操作</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#如果下一行没有内容的话，就会readline会返回-1，则while -1就会跳出循环</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> line:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">'---- processing '</span>, line_num, <span class="string">' article----------------'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    line_seg = <span class="string">" "</span>.join(jieba.cut(line))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    target.writelines(line_seg)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    line_num = line_num + <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    line = f.readline()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#关闭两个文件流，并退出程序</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">f.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">target.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">exit()</span></pre></td></tr></table></figure>

<h5 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2.训练模型"></a>2.训练模型</h5><p>有了分好词的语料，我们就可以通过Gensim模块中的word2vec函数来训练语料</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#程序的入口</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#1.如果当前脚本文件做模块供其他程序使用的话，不会执行if __name__ == '__main__':中的内容</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#2.如果直接执行当前的额脚本文件的话，执行if __name__ == '__main__':中的内容</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#1.os.path.basename('g://tf/code') ==&gt;code</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#2.sys.argv[0]获取的是脚本文件的文件名称</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    program = os.path.basename(sys.argv[<span class="number">0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#指定name，返回一个名称为name的Logger实例</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logger = logging.getLogger(program)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#1.format: 指定输出的格式和内容，format可以输出很多有用信息，</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(asctime)s: 打印日志的时间</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(levelname)s: 打印日志级别名称</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(message)s: 打印日志信息</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    logging.root.setLevel(level=logging.INFO)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#打印这是一个通知日志</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logger.info(<span class="string">"running %s"</span> % <span class="string">' '</span>.join(sys.argv))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># check and process input arguments</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">4</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> (globals()[<span class="string">'__doc__'</span>] % locals())</span></pre></td></tr><tr><td class="code"><pre><span class="line">        sys.exit(<span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#inp:分好词的文本</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#outp1:训练好的模型</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#outp2:得到的词向量</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    inp, outp1, outp2 = sys.argv[<span class="number">1</span>:<span class="number">4</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'''</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    size：是每个词的向量维度； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    sg (&#123;0, 1&#125;, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    alpha (float, optional) – 初始学习率</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    iter (int, optional) – 迭代次数，默认为5</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    '''</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model = Word2Vec(LineSentence(inp), size=<span class="number">400</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=multiprocessing.cpu_count())</span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.save(outp1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#不以C语言可以解析的形式存储词向量</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.wv.save_word2vec_format(outp2, binary=<span class="literal">False</span>)</span></pre></td></tr></table></figure>

<p>这里有一些训练词向量的调参技巧：</p>
<ol>
<li>选择的训练word2vec的语料要和要使用词向量的任务相似，并且越大越好，论文中实验说明语料比训练词向量的模型更加的重要，所以要尽量收集大的且与任务相关的语料来训练词向量；</li>
<li>语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型；</li>
<li>设置迭代次数为三五十次，维度至少选 50，常见的词向量的维度为256、512以及处理非常大的词表的时候的1024维；</li>
</ol>
<p>通过下面命令来执行Python文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python word2vec_model.py seg_filename model_name word2vec.vector</span></pre></td></tr></table></figure>

<ol>
<li>word2vec_model.py：存放训练代码的Python文件；</li>
<li>seg_filename：分好词的训练语料；</li>
<li>model_name：训练好的模型的名称；</li>
<li>word2vec.vector：得到的词向量；</li>
</ol>
<p>对于本例来说：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python word2vec_model.py wiki_jian_zh_seg<span class="number">-189.5</span>.txt wiki_zh_jian_text.model wiki_zh_jian_text.vector</span></pre></td></tr></table></figure>

<h2 id="3-测试模型"><a href="#3-测试模型" class="headerlink" title="3.测试模型"></a><strong>3.测试模型</strong></h2><p>有了词向量我们就可以使用词向量来做一些自然语言处理的任务了。那在这之前，我们需要测试一个模型是否训练成功。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from gensim.models import Word2Vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">en_wiki_word2vec_model &#x3D; Word2Vec.load(&#39;wiki_zh_jian_text.model&#39;)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">testwords &#x3D; [&#39;金融&#39;,&#39;上&#39;,&#39;股票&#39;,&#39;跌&#39;,&#39;经济&#39;]</span></pre></td></tr><tr><td class="code"><pre><span class="line">for i in range(5):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    res &#x3D; en_wiki_word2vec_model.most_similar(testwords[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print (testwords[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print (res)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">&#39;&#39;&#39;</span></pre></td></tr><tr><td class="code"><pre><span class="line">result:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    金融</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;金融业&#39;, 0.7712020874023438), (&#39;房地产&#39;, 0.7530461549758911), (&#39;银行业&#39;, 0.7478024959564209), (&#39;保险业&#39;, 0.7240537405014038), (&#39;金融机构&#39;, 0.7114974856376648), (&#39;投资银行&#39;, 0.7104595899581909), (&#39;证券&#39;, 0.7046274542808533), (&#39;信贷&#39;, 0.7021963596343994), (&#39;金融服务&#39;, 0.6956385374069214), (&#39;公共事业&#39;, 0.6882480382919312)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    上</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;之上&#39;, 0.5678470134735107), (&#39;上以&#39;, 0.4623713493347168), (&#39;上面&#39;, 0.4558977782726288), (&#39;上用&#39;, 0.42831096053123474), (&#39;水性&#39;, 0.4084252119064331), (&#39;上会&#39;, 0.3999699354171753), (&#39;方面&#39;, 0.3975197672843933), (&#39;上要&#39;, 0.3963406980037689), (&#39;上仅&#39;, 0.3950901925563812), (&#39;面上&#39;, 0.38935011625289917)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    股票</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;期货&#39;, 0.7636638879776001), (&#39;债券&#39;, 0.7634198069572449), (&#39;外汇&#39;, 0.7477541565895081), (&#39;获利&#39;, 0.7359930276870728), (&#39;期权&#39;, 0.7332447171211243), (&#39;A股&#39;, 0.7319167852401733), (&#39;存款&#39;, 0.7306094765663147), (&#39;普通股&#39;, 0.7264690399169922), (&#39;不动产&#39;, 0.724310040473938), (&#39;证券&#39;, 0.7240179777145386)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    跌</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;滑落&#39;, 0.70113605260849), (&#39;回落&#39;, 0.6962391138076782), (&#39;涨&#39;, 0.6842378377914429), (&#39;季尾&#39;, 0.6791133284568787), (&#39;攀升&#39;, 0.6673789620399475), (&#39;急跌&#39;, 0.6652034521102905), (&#39;跌落&#39;, 0.6540493965148926), (&#39;飙升&#39;, 0.6493663787841797), (&#39;下跌&#39;, 0.6452913284301758), (&#39;回升&#39;, 0.6349585652351379)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    经济</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;工商业&#39;, 0.631495475769043), (&#39;国民经济&#39;, 0.6289297342300415), (&#39;农业&#39;, 0.6132777333259583), (&#39;生产力&#39;, 0.6094485521316528), (&#39;金融&#39;, 0.5886996984481812), (&#39;市场经济&#39;, 0.5880722403526306), (&#39;旅游业&#39;, 0.585972011089325), (&#39;对外贸易&#39;, 0.575571596622467), (&#39;经济繁荣&#39;, 0.5738641023635864), (&#39;金融业&#39;, 0.5717495083808899)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">&#39;&#39;&#39;</span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title>使用自己的语料训练word2vec模型</title>
    <url>/2019/12/12/1/</url>
    <content><![CDATA[<h4 id="使用自己的语料训练word2vec模型"><a href="#使用自己的语料训练word2vec模型" class="headerlink" title="使用自己的语料训练word2vec模型"></a><strong>使用自己的语料训练word2vec模型</strong></h4><h5 id="一、准备环境和语料："><a href="#一、准备环境和语料：" class="headerlink" title="一、准备环境和语料："></a>一、准备环境和语料：</h5><p>新闻20w+篇（格式：<code>标题</code>。<code>正文</code>） </p>
<p>【新闻可以自己从各大新闻网站爬取，也可以下载开源的新闻数据集，如 </p>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5zb2dvdS5jb20vbGFicy9yZXNvdXJjZS90LnBocA==" title="http://www.sogou.com/labs/resource/t.php">互联网语料库(SogouT)<i class="fa fa-external-link"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3RodWN0Yy50aHVubHAub3JnLw==" title="http://thuctc.thunlp.org/">中文文本分类数据集THUCNews<i class="fa fa-external-link"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk2OA==" title="http://www.datatang.com/data/11968">李荣陆英文文本分类语料<i class="fa fa-external-link"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk3MA==" title="http://www.datatang.com/data/11970">谭松波中文文本分类语料<i class="fa fa-external-link"></i></span></li>
<li>等</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z4c2p5L2ppZWJh" title="https://github.com/fxsjy/jieba">结巴分词<i class="fa fa-external-link"></i></span> </li>
<li><span class="exturl" data-url="aHR0cHM6Ly9yYWRpbXJlaHVyZWsuY29tL2dlbnNpbS9tb2RlbHMvd29yZDJ2ZWMuaHRtbA==" title="https://radimrehurek.com/gensim/models/word2vec.html">word2vec<i class="fa fa-external-link"></i></span></li>
</ul>
<a id="more"></a>

<h5 id="二、分词"><a href="#二、分词" class="headerlink" title="二、分词"></a>二、分词</h5><p>先对新闻文本进行分词，使用的是结巴分词工具，将分词后的文本保存在<code>seg201708.txt</code>，以备后期使用。</p>
<blockquote>
<p>   安装jieba工具包：pip install jieba </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># 加载自己的自己的金融词库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">"financialWords.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">with</span> io.open(<span class="string">'news201708.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            seg_list = jieba.cut(line)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#           print '/'.join(seg_list)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">with</span> io.open(<span class="string">'seg201708.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                output.write(<span class="string">' '</span>.join(seg_list))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>



<h5 id="三、训练word2vec模型"><a href="#三、训练word2vec模型" class="headerlink" title="三、训练word2vec模型"></a>三、训练word2vec模型</h5><p>使用python的gensim包进行训练。</p>
<blockquote>
<p>   安装gemsim包：pip install gemsim </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_features = <span class="number">300</span>    <span class="comment"># Word vector dimensionality</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    min_word_count = <span class="number">10</span>   <span class="comment"># Minimum word count</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_workers = <span class="number">16</span>       <span class="comment"># Number of threads to run in parallel</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    context = <span class="number">10</span>          <span class="comment"># Context window size</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    downsampling = <span class="number">1e-3</span>   <span class="comment"># Downsample setting for frequent words</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    sentences = word2vec.Text8Corpus(<span class="string">"seg201708.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="code"><pre><span class="line">    model = word2vec.Word2Vec(sentences, workers=num_workers, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            size=num_features, min_count = min_word_count, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            window = context, sg = <span class="number">1</span>, sample = downsampling)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.init_sims(replace=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 保存模型，供日後使用</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.save(<span class="string">"model201708"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 可以在加载模型之后使用另外的句子来进一步训练模型</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model = gensim.models.Word2Vec.load('/tmp/mymodel')</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model.train(more_sentences)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>

<ul>
<li>参数说明</li>
</ul>
<blockquote>
</blockquote>
<ul>
<li>sentences：可以是一个·ist，对于大语料集，建议使用BrownCorpus,Text8Corpus或ineSentence构建。</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。</li>
<li>size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。</li>
<li>window：表示当前词与预测词在一个句子中的最大距离是多少</li>
<li>alpha: 是学习速率</li>
<li>seed：用于随机数发生器。与初始化词向量有关。</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)</li>
<li>workers参数控制训练的并行数。</li>
<li>hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。</li>
<li>negative: 如果&gt;0,则会采用negativesamp·ing，用于设置多少个noise words</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数</li>
<li>iter： 迭代次数，默认为5</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的</li>
<li>sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。</li>
<li>batch_words：每一批的传递给线程的单词的数量，默认为10000</li>
</ul>
<h5 id="四、word2vec应用"><a href="#四、word2vec应用" class="headerlink" title="四、word2vec应用"></a>四、word2vec应用</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Word2Vec.load(<span class="string">'model201708'</span>)      <span class="comment">#模型讀取方式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.most_similar(positive=[<span class="string">'woman'</span>, <span class="string">'king'</span>], negative=[<span class="string">'man'</span>]) <span class="comment">#根据给定的条件推断相似词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.doesnt_match(<span class="string">"breakfast cereal dinner lunch"</span>.split()) <span class="comment">#寻找离群词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.similarity(<span class="string">'woman'</span>, <span class="string">'man'</span>) <span class="comment">#计算两个单词的相似度</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model[<span class="string">'computer'</span>] <span class="comment">#获取单词的词向量</span></span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Word</title>
    <url>/2019/12/11/1/</url>
    <content><![CDATA[<p> <img src="https://ftp.bmp.ovh/imgs/2019/12/46248c3148c47ae2.png" alt=""> </p>
]]></content>
  </entry>
</search>
