<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>使用Gensim模块训练词向量(维基百科预料)</title>
    <url>/2019/12/13/1/</url>
    <content><![CDATA[<p>如果在以词为基本单元输入的自然语言处理任务中，都避免不了使用词的表示，词的表示有很多种，这里使用的是词向量，word2vec是目前比较通用的训练词向量的工具，使用Gensim模块，可以使词向量的训练变得简单，word2vec常见的两种模型 (CBOW和Skip-Gram) 他们的输入以及输出都是以单词为基本单位的，只是他们对应的输入以及输出不一样： </p>
<ol>
<li><p>Skip-Gram models：输入为单个词，输出目标为多个上下文单词；</p>
</li>
<li><p>CBOW models：输入为多个上下文单词，输出目标为一个单词；</p>
<p>从上面可以看出，无论是Skip-Gram models还是CBOW models基本的单元都是词，那么我们获取到的语料，必须要经过分词处理以后才能用于词向量的训练语料。 </p>
</li>
</ol>
<h5 id="1-数据的处理"><a href="#1-数据的处理" class="headerlink" title="1.数据的处理"></a>1.数据的处理</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#以写的方式打开原始的简体中文语料库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">f=codecs.open(<span class="string">'zhwiki_jian_zh.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">"utf8"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#将分完词的语料写入到wiki_jian_zh_seg-189.5.txt文件中</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">target = codecs.open(<span class="string">"wiki_jian_zh_seg-189.5.txt"</span>, <span class="string">'w'</span>,encoding=<span class="string">"utf8"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'open files'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">line_num=<span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">line = f.readline()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#循环遍历每一行，并对这一行进行分词操作</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#如果下一行没有内容的话，就会readline会返回-1，则while -1就会跳出循环</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> line:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">'---- processing '</span>, line_num, <span class="string">' article----------------'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    line_seg = <span class="string">" "</span>.join(jieba.cut(line))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    target.writelines(line_seg)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    line_num = line_num + <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    line = f.readline()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#关闭两个文件流，并退出程序</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">f.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">target.close()</span></pre></td></tr><tr><td class="code"><pre><span class="line">exit()</span></pre></td></tr></table></figure>

<h5 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2.训练模型"></a>2.训练模型</h5><p>有了分好词的语料，我们就可以通过Gensim模块中的word2vec函数来训练语料</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#程序的入口</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#1.如果当前脚本文件做模块供其他程序使用的话，不会执行if __name__ == '__main__':中的内容</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#2.如果直接执行当前的额脚本文件的话，执行if __name__ == '__main__':中的内容</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#1.os.path.basename('g://tf/code') ==&gt;code</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#2.sys.argv[0]获取的是脚本文件的文件名称</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    program = os.path.basename(sys.argv[<span class="number">0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#指定name，返回一个名称为name的Logger实例</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logger = logging.getLogger(program)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#1.format: 指定输出的格式和内容，format可以输出很多有用信息，</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(asctime)s: 打印日志的时间</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(levelname)s: 打印日志级别名称</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#%(message)s: 打印日志信息</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    logging.root.setLevel(level=logging.INFO)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#打印这是一个通知日志</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    logger.info(<span class="string">"running %s"</span> % <span class="string">' '</span>.join(sys.argv))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># check and process input arguments</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">4</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">print</span> (globals()[<span class="string">'__doc__'</span>] % locals())</span></pre></td></tr><tr><td class="code"><pre><span class="line">        sys.exit(<span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#inp:分好词的文本</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#outp1:训练好的模型</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#outp2:得到的词向量</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    inp, outp1, outp2 = sys.argv[<span class="number">1</span>:<span class="number">4</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'''</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    size：是每个词的向量维度； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； </span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    sg (&#123;0, 1&#125;, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    alpha (float, optional) – 初始学习率</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    iter (int, optional) – 迭代次数，默认为5</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="string">    '''</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model = Word2Vec(LineSentence(inp), size=<span class="number">400</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=multiprocessing.cpu_count())</span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.save(outp1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">#不以C语言可以解析的形式存储词向量</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.wv.save_word2vec_format(outp2, binary=<span class="literal">False</span>)</span></pre></td></tr></table></figure>

<p>这里有一些训练词向量的调参技巧：</p>
<ol>
<li>选择的训练word2vec的语料要和要使用词向量的任务相似，并且越大越好，论文中实验说明语料比训练词向量的模型更加的重要，所以要尽量收集大的且与任务相关的语料来训练词向量；</li>
<li>语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型；</li>
<li>设置迭代次数为三五十次，维度至少选 50，常见的词向量的维度为256、512以及处理非常大的词表的时候的1024维；</li>
</ol>
<p>通过下面命令来执行Python文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python word2vec_model.py seg_filename model_name word2vec.vector</span></pre></td></tr></table></figure>

<ol>
<li>word2vec_model.py：存放训练代码的Python文件；</li>
<li>seg_filename：分好词的训练语料；</li>
<li>model_name：训练好的模型的名称；</li>
<li>word2vec.vector：得到的词向量；</li>
</ol>
<p>对于本例来说：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python word2vec_model.py wiki_jian_zh_seg<span class="number">-189.5</span>.txt wiki_zh_jian_text.model wiki_zh_jian_text.vector</span></pre></td></tr></table></figure>

<h2 id="3-测试模型"><a href="#3-测试模型" class="headerlink" title="3.测试模型"></a><strong>3.测试模型</strong></h2><p>有了词向量我们就可以使用词向量来做一些自然语言处理的任务了。那在这之前，我们需要测试一个模型是否训练成功。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from gensim.models import Word2Vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">en_wiki_word2vec_model &#x3D; Word2Vec.load(&#39;wiki_zh_jian_text.model&#39;)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">testwords &#x3D; [&#39;金融&#39;,&#39;上&#39;,&#39;股票&#39;,&#39;跌&#39;,&#39;经济&#39;]</span></pre></td></tr><tr><td class="code"><pre><span class="line">for i in range(5):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    res &#x3D; en_wiki_word2vec_model.most_similar(testwords[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print (testwords[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print (res)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">&#39;&#39;&#39;</span></pre></td></tr><tr><td class="code"><pre><span class="line">result:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    金融</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;金融业&#39;, 0.7712020874023438), (&#39;房地产&#39;, 0.7530461549758911), (&#39;银行业&#39;, 0.7478024959564209), (&#39;保险业&#39;, 0.7240537405014038), (&#39;金融机构&#39;, 0.7114974856376648), (&#39;投资银行&#39;, 0.7104595899581909), (&#39;证券&#39;, 0.7046274542808533), (&#39;信贷&#39;, 0.7021963596343994), (&#39;金融服务&#39;, 0.6956385374069214), (&#39;公共事业&#39;, 0.6882480382919312)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    上</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;之上&#39;, 0.5678470134735107), (&#39;上以&#39;, 0.4623713493347168), (&#39;上面&#39;, 0.4558977782726288), (&#39;上用&#39;, 0.42831096053123474), (&#39;水性&#39;, 0.4084252119064331), (&#39;上会&#39;, 0.3999699354171753), (&#39;方面&#39;, 0.3975197672843933), (&#39;上要&#39;, 0.3963406980037689), (&#39;上仅&#39;, 0.3950901925563812), (&#39;面上&#39;, 0.38935011625289917)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    股票</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;期货&#39;, 0.7636638879776001), (&#39;债券&#39;, 0.7634198069572449), (&#39;外汇&#39;, 0.7477541565895081), (&#39;获利&#39;, 0.7359930276870728), (&#39;期权&#39;, 0.7332447171211243), (&#39;A股&#39;, 0.7319167852401733), (&#39;存款&#39;, 0.7306094765663147), (&#39;普通股&#39;, 0.7264690399169922), (&#39;不动产&#39;, 0.724310040473938), (&#39;证券&#39;, 0.7240179777145386)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    跌</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;滑落&#39;, 0.70113605260849), (&#39;回落&#39;, 0.6962391138076782), (&#39;涨&#39;, 0.6842378377914429), (&#39;季尾&#39;, 0.6791133284568787), (&#39;攀升&#39;, 0.6673789620399475), (&#39;急跌&#39;, 0.6652034521102905), (&#39;跌落&#39;, 0.6540493965148926), (&#39;飙升&#39;, 0.6493663787841797), (&#39;下跌&#39;, 0.6452913284301758), (&#39;回升&#39;, 0.6349585652351379)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    经济</span></pre></td></tr><tr><td class="code"><pre><span class="line">        [(&#39;工商业&#39;, 0.631495475769043), (&#39;国民经济&#39;, 0.6289297342300415), (&#39;农业&#39;, 0.6132777333259583), (&#39;生产力&#39;, 0.6094485521316528), (&#39;金融&#39;, 0.5886996984481812), (&#39;市场经济&#39;, 0.5880722403526306), (&#39;旅游业&#39;, 0.585972011089325), (&#39;对外贸易&#39;, 0.575571596622467), (&#39;经济繁荣&#39;, 0.5738641023635864), (&#39;金融业&#39;, 0.5717495083808899)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">&#39;&#39;&#39;</span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title>使用自己的语料训练word2vec模型</title>
    <url>/2019/12/12/1/</url>
    <content><![CDATA[<h4 id="使用自己的语料训练word2vec模型"><a href="#使用自己的语料训练word2vec模型" class="headerlink" title="使用自己的语料训练word2vec模型"></a><strong>使用自己的语料训练word2vec模型</strong></h4><h5 id="一、准备环境和语料："><a href="#一、准备环境和语料：" class="headerlink" title="一、准备环境和语料："></a>一、准备环境和语料：</h5><p>新闻20w+篇（格式：<code>标题</code>。<code>正文</code>） </p>
<p>【新闻可以自己从各大新闻网站爬取，也可以下载开源的新闻数据集，如 </p>
<ul>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5zb2dvdS5jb20vbGFicy9yZXNvdXJjZS90LnBocA==" title="http://www.sogou.com/labs/resource/t.php">互联网语料库(SogouT)<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3RodWN0Yy50aHVubHAub3JnLw==" title="http://thuctc.thunlp.org/">中文文本分类数据集THUCNews<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk2OA==" title="http://www.datatang.com/data/11968">李荣陆英文文本分类语料<i class="fa fa-external-link"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3d3dy5kYXRhdGFuZy5jb20vZGF0YS8xMTk3MA==" title="http://www.datatang.com/data/11970">谭松波中文文本分类语料<i class="fa fa-external-link"></i></span></p>
</li>
<li><p>等</p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z4c2p5L2ppZWJh" title="https://github.com/fxsjy/jieba">结巴分词<i class="fa fa-external-link"></i></span> </p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9yYWRpbXJlaHVyZWsuY29tL2dlbnNpbS9tb2RlbHMvd29yZDJ2ZWMuaHRtbA==" title="https://radimrehurek.com/gensim/models/word2vec.html">word2vec<i class="fa fa-external-link"></i></span></p>
</li>
</ul>
<h5 id="二、分词"><a href="#二、分词" class="headerlink" title="二、分词"></a>二、分词</h5><p>先对新闻文本进行分词，使用的是结巴分词工具，将分词后的文本保存在<code>seg201708.txt</code>，以备后期使用。</p>
<blockquote>
<p>   安装jieba工具包：pip install jieba </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># 加载自己的自己的金融词库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">"financialWords.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">with</span> io.open(<span class="string">'news201708.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span></pre></td></tr><tr><td class="code"><pre><span class="line">            seg_list = jieba.cut(line)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#           print '/'.join(seg_list)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">            <span class="keyword">with</span> io.open(<span class="string">'seg201708.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output:</span></pre></td></tr><tr><td class="code"><pre><span class="line">                output.write(<span class="string">' '</span>.join(seg_list))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>

<a id="more"></a>

<h5 id="三、训练word2vec模型"><a href="#三、训练word2vec模型" class="headerlink" title="三、训练word2vec模型"></a>三、训练word2vec模型</h5><p>使用python的gensim包进行训练。</p>
<blockquote>
<p>   安装gemsim包：pip install gemsim </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_features = <span class="number">300</span>    <span class="comment"># Word vector dimensionality</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    min_word_count = <span class="number">10</span>   <span class="comment"># Minimum word count</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    num_workers = <span class="number">16</span>       <span class="comment"># Number of threads to run in parallel</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    context = <span class="number">10</span>          <span class="comment"># Context window size</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    downsampling = <span class="number">1e-3</span>   <span class="comment"># Downsample setting for frequent words</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    sentences = word2vec.Text8Corpus(<span class="string">"seg201708.txt"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="code"><pre><span class="line">    model = word2vec.Word2Vec(sentences, workers=num_workers, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            size=num_features, min_count = min_word_count, \</span></pre></td></tr><tr><td class="code"><pre><span class="line">            window = context, sg = <span class="number">1</span>, sample = downsampling)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.init_sims(replace=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 保存模型，供日後使用</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    model.save(<span class="string">"model201708"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># 可以在加载模型之后使用另外的句子来进一步训练模型</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model = gensim.models.Word2Vec.load('/tmp/mymodel')</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment"># model.train(more_sentences)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    main()</span></pre></td></tr></table></figure>

<ul>
<li>参数说明</li>
</ul>
<blockquote>
</blockquote>
<ul>
<li>sentences：可以是一个·ist，对于大语料集，建议使用BrownCorpus,Text8Corpus或ineSentence构建。</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。</li>
<li>size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。</li>
<li>window：表示当前词与预测词在一个句子中的最大距离是多少</li>
<li>alpha: 是学习速率</li>
<li>seed：用于随机数发生器。与初始化词向量有关。</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)</li>
<li>workers参数控制训练的并行数。</li>
<li>hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。</li>
<li>negative: 如果&gt;0,则会采用negativesamp·ing，用于设置多少个noise words</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数</li>
<li>iter： 迭代次数，默认为5</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的</li>
<li>sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。</li>
<li>batch_words：每一批的传递给线程的单词的数量，默认为10000</li>
</ul>
<h5 id="四、word2vec应用"><a href="#四、word2vec应用" class="headerlink" title="四、word2vec应用"></a>四、word2vec应用</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Word2Vec.load(<span class="string">'model201708'</span>)      <span class="comment">#模型讀取方式</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.most_similar(positive=[<span class="string">'woman'</span>, <span class="string">'king'</span>], negative=[<span class="string">'man'</span>]) <span class="comment">#根据给定的条件推断相似词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.doesnt_match(<span class="string">"breakfast cereal dinner lunch"</span>.split()) <span class="comment">#寻找离群词</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model.similarity(<span class="string">'woman'</span>, <span class="string">'man'</span>) <span class="comment">#计算两个单词的相似度</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">model[<span class="string">'computer'</span>] <span class="comment">#获取单词的词向量</span></span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Word</title>
    <url>/2019/12/11/1/</url>
    <content><![CDATA[<p> <img src="https://ftp.bmp.ovh/imgs/2019/12/46248c3148c47ae2.png" alt=""> </p>
]]></content>
  </entry>
</search>
